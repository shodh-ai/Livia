wandb: Currently logged in as: inferq_team (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.17.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.28
wandb: Syncing run 8da2f4074330-0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inferq_team/70M_CRMS_PreNorm_Harshit_fp16_zero1
wandb: üöÄ View run at https://wandb.ai/inferq_team/70M_CRMS_PreNorm_Harshit_fp16_zero1/runs/18vhytlo
wandb: Run data is saved locally in /workspace/My_Test/gpt-neox-custom/wandb/run-20240608_095647-18vhytlo
wandb: Run `wandb offline` to turn off syncing.
Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
  File "train.py", line 34, in <module>
    main()
  File "train.py", line 30, in main
    pretrain(neox_args=neox_args)
  File "/workspace/My_Test/gpt-neox-custom/megatron/training.py", line 230, in pretrain
    iteration = train(
  File "/workspace/My_Test/gpt-neox-custom/megatron/training.py", line 995, in train
    loss_dict, skipped_iter = train_step(
  File "/workspace/My_Test/gpt-neox-custom/megatron/training.py", line 915, in train_step
    model.step()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 2049, in step
    self._take_model_step(lr_kwargs)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/engine.py", line 1956, in _take_model_step
    self.optimizer.step()
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1658, in step
    self._update_scale(self.overflow)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1907, in _update_scale
    self.loss_scaler.update_scale(has_overflow)
  File "/usr/local/lib/python3.8/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 174, in update_scale
    raise Exception(
Exception: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.
wandb: Waiting for W&B process to finish, PID 3349
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /workspace/My_Test/gpt-neox-custom/wandb/run-20240608_095647-18vhytlo/logs/debug.log
wandb: Find internal logs for this run at: /workspace/My_Test/gpt-neox-custom/wandb/run-20240608_095647-18vhytlo/logs/debug-internal.log
wandb: Run summary:
wandb:                  timers/forward 0.03773
wandb:                 timers/backward 0.05115
wandb:        timers/backward-backward 0.05114
wandb:       timers/backward-allreduce 0.0
wandb:                timers/optimizer 0.00752
wandb:          timers/batch generator 0.00133
wandb:             train/learning_rate 0.00018
wandb:                   train/lm_loss nan
wandb:                train/loss_scale 1.0
wandb:                        _runtime 5996
wandb:                      _timestamp 1717846603
wandb:                           _step 86511
wandb:         runtime/samples_per_sec 38.04907
wandb:          runtime/iteration_time 0.10513
wandb:   runtime/flops_per_sec_per_gpu 20733849376622.246
wandb:              validation/lm_loss 6.14796
wandb:          validation/lm_loss_ppl 467.7607
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced 8da2f4074330-0: https://wandb.ai/inferq_team/70M_CRMS_PreNorm_Harshit_fp16_zero1/runs/18vhytlo
