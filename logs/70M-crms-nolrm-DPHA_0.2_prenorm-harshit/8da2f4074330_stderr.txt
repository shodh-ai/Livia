WARNING:root:Outstanding DeepSpeed issue means that pp>0, zero1, and bf16 will break without fp32 grads
wandb: Currently logged in as: inferq_team (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.17.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.28
wandb: Syncing run 8da2f4074330-0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inferq_team/70M_CRMS_NOLZM_DPHA_0.2_PreNorm_Harshit
wandb: üöÄ View run at https://wandb.ai/inferq_team/70M_CRMS_NOLZM_DPHA_0.2_PreNorm_Harshit/runs/16ywv1yo
wandb: Run data is saved locally in /workspace/My_Test/gpt-neox-custom/wandb/run-20240611_055856-16ywv1yo
wandb: Run `wandb offline` to turn off syncing.
Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /root/.cache/torch_extensions/py38_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module utils...
Using /root/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1432: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
wandb: Waiting for W&B process to finish, PID 1410
wandb: Program ended successfully.
wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: \ 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: | 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: / 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb: - 0.03MB of 0.03MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /workspace/My_Test/gpt-neox-custom/wandb/run-20240611_055856-16ywv1yo/logs/debug.log
wandb: Find internal logs for this run at: /workspace/My_Test/gpt-neox-custom/wandb/run-20240611_055856-16ywv1yo/logs/debug-internal.log
wandb: Run summary:
wandb:                  timers/forward 0.03819
wandb:                 timers/backward 0.05218
wandb:        timers/backward-backward 0.05217
wandb:       timers/backward-allreduce 0.0
wandb:                timers/optimizer 0.00769
wandb:          timers/batch generator 0.00074
wandb:             train/learning_rate 2e-05
wandb:                   train/lm_loss 4.78553
wandb:                        _runtime 14485
wandb:                      _timestamp 1718100021
wandb:                           _step 350000
wandb:         runtime/samples_per_sec 38.80489
wandb:          runtime/iteration_time 0.10308
wandb:   runtime/flops_per_sec_per_gpu 21145717625066.707
wandb:              validation/lm_loss 5.98799
wandb:          validation/lm_loss_ppl 398.61388
wandb:                    test/lm_loss 5.9005
wandb:                test/lm_loss_ppl 365.22014
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced 8da2f4074330-0: https://wandb.ai/inferq_team/70M_CRMS_NOLZM_DPHA_0.2_PreNorm_Harshit/runs/16ywv1yo
